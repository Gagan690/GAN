{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "AFniRJSkxjae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 100"
      ],
      "metadata": {
        "id": "IIrf-U2zzuyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5)) # Changed to single values for mean and std\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "id": "nFeOGAeCx6N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))"
      ],
      "metadata": {
        "id": "hpcDmueQyT5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build network\n",
        "z_dim = 100\n",
        "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
        "\n",
        "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
        "D = Discriminator(mnist_dim).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4q1HHUJzlge",
        "outputId": "26526edb-75e3-454e-a62a-dbf89b010121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TgkSUyfzxjX",
        "outputId": "75d03962-61e5-4b92-ab01-ba5026595146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (fc1): Linear(in_features=100, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  (fc4): Linear(in_features=1024, out_features=784, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M88qAtGtzz8B",
        "outputId": "6d6153ac-a9a9-4d0c-92ea-33584f89d9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# optimizer\n",
        "lr = 0.0002\n",
        "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "XxrUv1ewz1Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def D_train(x):\n",
        "    #=======================Train the discriminator=======================#\n",
        "    D.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    x_real, y_real = x.view(-1, mnist_dim), torch.ones(bs, 1)\n",
        "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
        "\n",
        "    D_output = D(x_real)\n",
        "    D_real_loss = criterion(D_output, y_real)\n",
        "    D_real_score = D_output\n",
        "\n",
        "    # train discriminator on facke\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device))\n",
        "\n",
        "    D_output = D(x_fake)\n",
        "    D_fake_loss = criterion(D_output, y_fake)\n",
        "    D_fake_score = D_output\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = D_real_loss + D_fake_loss\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    return  D_loss.data.item()"
      ],
      "metadata": {
        "id": "VE1VCb0Pz6iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def G_train(x):\n",
        "    #=======================Train the generator=======================#\n",
        "    G.zero_grad()\n",
        "\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    y = Variable(torch.ones(bs, 1).to(device))\n",
        "\n",
        "    G_output = G(z)\n",
        "    D_output = D(G_output)\n",
        "    G_loss = criterion(D_output, y)\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    return G_loss.data.item()"
      ],
      "metadata": {
        "id": "mpObVJAvz8yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 200\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    D_losses, G_losses = [], []\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        D_losses.append(D_train(x))\n",
        "        G_losses.append(G_train(x))\n",
        "\n",
        "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7wmgb8Xz_4G",
        "outputId": "06cf0947-b406-48d0-8252-be1c10475ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/200]: loss_d: 0.997, loss_g: 2.470\n",
            "[2/200]: loss_d: 1.120, loss_g: 1.474\n",
            "[3/200]: loss_d: 1.054, loss_g: 1.590\n",
            "[4/200]: loss_d: 0.929, loss_g: 2.195\n",
            "[5/200]: loss_d: 0.672, loss_g: 2.364\n",
            "[6/200]: loss_d: 0.527, loss_g: 2.773\n",
            "[7/200]: loss_d: 0.610, loss_g: 2.551\n",
            "[8/200]: loss_d: 0.652, loss_g: 2.356\n",
            "[9/200]: loss_d: 0.633, loss_g: 2.437\n",
            "[10/200]: loss_d: 0.670, loss_g: 2.403\n",
            "[11/200]: loss_d: 0.594, loss_g: 2.537\n",
            "[12/200]: loss_d: 0.648, loss_g: 2.434\n",
            "[13/200]: loss_d: 0.693, loss_g: 2.289\n",
            "[14/200]: loss_d: 0.744, loss_g: 2.084\n",
            "[15/200]: loss_d: 0.744, loss_g: 2.080\n",
            "[16/200]: loss_d: 0.722, loss_g: 2.119\n",
            "[17/200]: loss_d: 0.745, loss_g: 2.006\n",
            "[18/200]: loss_d: 0.811, loss_g: 1.828\n",
            "[19/200]: loss_d: 0.831, loss_g: 1.832\n",
            "[20/200]: loss_d: 0.880, loss_g: 1.640\n",
            "[21/200]: loss_d: 0.916, loss_g: 1.597\n",
            "[22/200]: loss_d: 0.954, loss_g: 1.502\n",
            "[23/200]: loss_d: 0.915, loss_g: 1.562\n",
            "[24/200]: loss_d: 0.974, loss_g: 1.460\n",
            "[25/200]: loss_d: 0.958, loss_g: 1.509\n",
            "[26/200]: loss_d: 0.934, loss_g: 1.562\n",
            "[27/200]: loss_d: 0.938, loss_g: 1.537\n",
            "[28/200]: loss_d: 0.982, loss_g: 1.443\n",
            "[29/200]: loss_d: 0.985, loss_g: 1.431\n",
            "[30/200]: loss_d: 1.020, loss_g: 1.349\n",
            "[31/200]: loss_d: 1.034, loss_g: 1.320\n",
            "[32/200]: loss_d: 1.027, loss_g: 1.352\n",
            "[33/200]: loss_d: 1.014, loss_g: 1.368\n",
            "[34/200]: loss_d: 1.051, loss_g: 1.301\n",
            "[35/200]: loss_d: 1.066, loss_g: 1.266\n",
            "[36/200]: loss_d: 1.063, loss_g: 1.253\n",
            "[37/200]: loss_d: 1.072, loss_g: 1.262\n",
            "[38/200]: loss_d: 1.075, loss_g: 1.244\n",
            "[39/200]: loss_d: 1.074, loss_g: 1.252\n",
            "[40/200]: loss_d: 1.115, loss_g: 1.170\n",
            "[41/200]: loss_d: 1.096, loss_g: 1.204\n",
            "[42/200]: loss_d: 1.121, loss_g: 1.164\n",
            "[43/200]: loss_d: 1.111, loss_g: 1.179\n",
            "[44/200]: loss_d: 1.130, loss_g: 1.143\n",
            "[45/200]: loss_d: 1.135, loss_g: 1.131\n",
            "[46/200]: loss_d: 1.149, loss_g: 1.121\n",
            "[47/200]: loss_d: 1.138, loss_g: 1.119\n",
            "[48/200]: loss_d: 1.145, loss_g: 1.130\n",
            "[49/200]: loss_d: 1.149, loss_g: 1.111\n",
            "[50/200]: loss_d: 1.159, loss_g: 1.084\n",
            "[51/200]: loss_d: 1.167, loss_g: 1.077\n",
            "[52/200]: loss_d: 1.173, loss_g: 1.067\n",
            "[53/200]: loss_d: 1.164, loss_g: 1.072\n",
            "[54/200]: loss_d: 1.182, loss_g: 1.061\n",
            "[55/200]: loss_d: 1.183, loss_g: 1.047\n",
            "[56/200]: loss_d: 1.190, loss_g: 1.037\n",
            "[57/200]: loss_d: 1.185, loss_g: 1.049\n",
            "[58/200]: loss_d: 1.183, loss_g: 1.045\n",
            "[59/200]: loss_d: 1.197, loss_g: 1.019\n",
            "[60/200]: loss_d: 1.194, loss_g: 1.039\n",
            "[61/200]: loss_d: 1.187, loss_g: 1.052\n",
            "[62/200]: loss_d: 1.203, loss_g: 1.018\n",
            "[63/200]: loss_d: 1.201, loss_g: 1.020\n",
            "[64/200]: loss_d: 1.203, loss_g: 1.023\n",
            "[65/200]: loss_d: 1.193, loss_g: 1.026\n",
            "[66/200]: loss_d: 1.207, loss_g: 1.002\n",
            "[67/200]: loss_d: 1.215, loss_g: 0.991\n",
            "[68/200]: loss_d: 1.215, loss_g: 0.986\n",
            "[69/200]: loss_d: 1.216, loss_g: 0.992\n",
            "[70/200]: loss_d: 1.209, loss_g: 0.996\n",
            "[71/200]: loss_d: 1.213, loss_g: 0.997\n",
            "[72/200]: loss_d: 1.220, loss_g: 0.975\n",
            "[73/200]: loss_d: 1.217, loss_g: 0.988\n",
            "[74/200]: loss_d: 1.224, loss_g: 0.971\n",
            "[75/200]: loss_d: 1.213, loss_g: 0.996\n",
            "[76/200]: loss_d: 1.223, loss_g: 0.980\n",
            "[77/200]: loss_d: 1.228, loss_g: 0.971\n",
            "[78/200]: loss_d: 1.235, loss_g: 0.955\n",
            "[79/200]: loss_d: 1.239, loss_g: 0.957\n",
            "[80/200]: loss_d: 1.239, loss_g: 0.956\n",
            "[81/200]: loss_d: 1.225, loss_g: 0.968\n",
            "[82/200]: loss_d: 1.237, loss_g: 0.948\n",
            "[83/200]: loss_d: 1.233, loss_g: 0.969\n",
            "[84/200]: loss_d: 1.229, loss_g: 0.970\n",
            "[85/200]: loss_d: 1.239, loss_g: 0.955\n",
            "[86/200]: loss_d: 1.243, loss_g: 0.940\n",
            "[87/200]: loss_d: 1.243, loss_g: 0.945\n",
            "[88/200]: loss_d: 1.238, loss_g: 0.961\n",
            "[89/200]: loss_d: 1.243, loss_g: 0.949\n",
            "[90/200]: loss_d: 1.240, loss_g: 0.948\n",
            "[91/200]: loss_d: 1.250, loss_g: 0.929\n",
            "[92/200]: loss_d: 1.252, loss_g: 0.929\n",
            "[93/200]: loss_d: 1.252, loss_g: 0.930\n",
            "[94/200]: loss_d: 1.252, loss_g: 0.941\n",
            "[95/200]: loss_d: 1.249, loss_g: 0.932\n",
            "[96/200]: loss_d: 1.258, loss_g: 0.913\n",
            "[97/200]: loss_d: 1.257, loss_g: 0.923\n",
            "[98/200]: loss_d: 1.247, loss_g: 0.940\n",
            "[99/200]: loss_d: 1.250, loss_g: 0.934\n",
            "[100/200]: loss_d: 1.256, loss_g: 0.920\n",
            "[101/200]: loss_d: 1.255, loss_g: 0.922\n",
            "[102/200]: loss_d: 1.260, loss_g: 0.923\n",
            "[103/200]: loss_d: 1.260, loss_g: 0.920\n",
            "[104/200]: loss_d: 1.258, loss_g: 0.915\n",
            "[105/200]: loss_d: 1.254, loss_g: 0.928\n",
            "[106/200]: loss_d: 1.262, loss_g: 0.918\n",
            "[107/200]: loss_d: 1.261, loss_g: 0.917\n",
            "[108/200]: loss_d: 1.261, loss_g: 0.914\n",
            "[109/200]: loss_d: 1.263, loss_g: 0.914\n",
            "[110/200]: loss_d: 1.269, loss_g: 0.896\n",
            "[111/200]: loss_d: 1.268, loss_g: 0.906\n",
            "[112/200]: loss_d: 1.271, loss_g: 0.898\n",
            "[113/200]: loss_d: 1.261, loss_g: 0.909\n",
            "[114/200]: loss_d: 1.263, loss_g: 0.917\n",
            "[115/200]: loss_d: 1.264, loss_g: 0.903\n",
            "[116/200]: loss_d: 1.271, loss_g: 0.904\n",
            "[117/200]: loss_d: 1.268, loss_g: 0.906\n",
            "[118/200]: loss_d: 1.270, loss_g: 0.891\n",
            "[119/200]: loss_d: 1.268, loss_g: 0.903\n",
            "[120/200]: loss_d: 1.265, loss_g: 0.912\n",
            "[121/200]: loss_d: 1.258, loss_g: 0.924\n",
            "[122/200]: loss_d: 1.273, loss_g: 0.889\n",
            "[123/200]: loss_d: 1.279, loss_g: 0.888\n",
            "[124/200]: loss_d: 1.272, loss_g: 0.896\n",
            "[125/200]: loss_d: 1.280, loss_g: 0.884\n",
            "[126/200]: loss_d: 1.277, loss_g: 0.895\n",
            "[127/200]: loss_d: 1.273, loss_g: 0.886\n",
            "[128/200]: loss_d: 1.276, loss_g: 0.888\n",
            "[129/200]: loss_d: 1.274, loss_g: 0.892\n",
            "[130/200]: loss_d: 1.281, loss_g: 0.880\n",
            "[131/200]: loss_d: 1.280, loss_g: 0.891\n",
            "[132/200]: loss_d: 1.271, loss_g: 0.896\n",
            "[133/200]: loss_d: 1.272, loss_g: 0.895\n",
            "[134/200]: loss_d: 1.274, loss_g: 0.890\n",
            "[135/200]: loss_d: 1.275, loss_g: 0.894\n",
            "[136/200]: loss_d: 1.277, loss_g: 0.889\n",
            "[137/200]: loss_d: 1.277, loss_g: 0.890\n",
            "[138/200]: loss_d: 1.276, loss_g: 0.889\n",
            "[139/200]: loss_d: 1.282, loss_g: 0.877\n",
            "[140/200]: loss_d: 1.276, loss_g: 0.893\n",
            "[141/200]: loss_d: 1.278, loss_g: 0.881\n",
            "[142/200]: loss_d: 1.285, loss_g: 0.879\n",
            "[143/200]: loss_d: 1.286, loss_g: 0.872\n",
            "[144/200]: loss_d: 1.282, loss_g: 0.884\n",
            "[145/200]: loss_d: 1.287, loss_g: 0.871\n",
            "[146/200]: loss_d: 1.283, loss_g: 0.874\n",
            "[147/200]: loss_d: 1.287, loss_g: 0.871\n",
            "[148/200]: loss_d: 1.287, loss_g: 0.874\n",
            "[149/200]: loss_d: 1.290, loss_g: 0.871\n",
            "[150/200]: loss_d: 1.282, loss_g: 0.894\n",
            "[151/200]: loss_d: 1.277, loss_g: 0.893\n",
            "[152/200]: loss_d: 1.280, loss_g: 0.885\n",
            "[153/200]: loss_d: 1.282, loss_g: 0.883\n",
            "[154/200]: loss_d: 1.291, loss_g: 0.865\n",
            "[155/200]: loss_d: 1.280, loss_g: 0.883\n",
            "[156/200]: loss_d: 1.282, loss_g: 0.874\n",
            "[157/200]: loss_d: 1.289, loss_g: 0.870\n",
            "[158/200]: loss_d: 1.285, loss_g: 0.875\n",
            "[159/200]: loss_d: 1.288, loss_g: 0.870\n",
            "[160/200]: loss_d: 1.283, loss_g: 0.878\n",
            "[161/200]: loss_d: 1.285, loss_g: 0.871\n",
            "[162/200]: loss_d: 1.286, loss_g: 0.866\n",
            "[163/200]: loss_d: 1.290, loss_g: 0.869\n",
            "[164/200]: loss_d: 1.290, loss_g: 0.865\n",
            "[165/200]: loss_d: 1.286, loss_g: 0.883\n",
            "[166/200]: loss_d: 1.286, loss_g: 0.872\n",
            "[167/200]: loss_d: 1.288, loss_g: 0.865\n",
            "[168/200]: loss_d: 1.288, loss_g: 0.868\n",
            "[169/200]: loss_d: 1.285, loss_g: 0.877\n",
            "[170/200]: loss_d: 1.286, loss_g: 0.869\n",
            "[171/200]: loss_d: 1.289, loss_g: 0.864\n",
            "[172/200]: loss_d: 1.287, loss_g: 0.869\n",
            "[173/200]: loss_d: 1.290, loss_g: 0.868\n",
            "[174/200]: loss_d: 1.283, loss_g: 0.876\n",
            "[175/200]: loss_d: 1.287, loss_g: 0.878\n",
            "[176/200]: loss_d: 1.283, loss_g: 0.880\n",
            "[177/200]: loss_d: 1.288, loss_g: 0.874\n",
            "[178/200]: loss_d: 1.285, loss_g: 0.878\n",
            "[179/200]: loss_d: 1.288, loss_g: 0.874\n",
            "[180/200]: loss_d: 1.291, loss_g: 0.863\n",
            "[181/200]: loss_d: 1.296, loss_g: 0.846\n",
            "[182/200]: loss_d: 1.288, loss_g: 0.875\n",
            "[183/200]: loss_d: 1.292, loss_g: 0.868\n",
            "[184/200]: loss_d: 1.294, loss_g: 0.855\n",
            "[185/200]: loss_d: 1.291, loss_g: 0.870\n",
            "[186/200]: loss_d: 1.290, loss_g: 0.863\n",
            "[187/200]: loss_d: 1.289, loss_g: 0.877\n",
            "[188/200]: loss_d: 1.297, loss_g: 0.851\n",
            "[189/200]: loss_d: 1.295, loss_g: 0.858\n",
            "[190/200]: loss_d: 1.300, loss_g: 0.859\n",
            "[191/200]: loss_d: 1.295, loss_g: 0.860\n",
            "[192/200]: loss_d: 1.295, loss_g: 0.858\n",
            "[193/200]: loss_d: 1.290, loss_g: 0.871\n",
            "[194/200]: loss_d: 1.295, loss_g: 0.866\n",
            "[195/200]: loss_d: 1.294, loss_g: 0.860\n",
            "[196/200]: loss_d: 1.287, loss_g: 0.874\n",
            "[197/200]: loss_d: 1.287, loss_g: 0.866\n",
            "[198/200]: loss_d: 1.299, loss_g: 0.852\n",
            "[199/200]: loss_d: 1.296, loss_g: 0.857\n",
            "[200/200]: loss_d: 1.297, loss_g: 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate a random noise vector\n",
        "z = torch.randn(1, z_dim).to(device)\n",
        "\n",
        "# Generate an image using the generator\n",
        "G.eval() # Set the generator to evaluation mode\n",
        "with torch.no_grad():\n",
        "    generated_image = G(z).view(28, 28).cpu().numpy()\n",
        "\n",
        "# Display the generated image\n",
        "plt.imshow(generated_image, cmap='gray')\n",
        "plt.title(\"Generated Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "xx8-Xgok0a7D",
        "outputId": "2472cb51-5d8c-4df3-eec3-d5dbff574878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the path to save the model\n",
        "model_save_path = 'generator_model.pth'\n",
        "\n",
        "# Save the generator's state dictionary\n",
        "torch.save(G.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Generator model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptmG4Cai-qPs",
        "outputId": "0655a5ed-8cfc-4f5e-ac95-943c6e1c98e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator model saved to generator_model.pth\n"
          ]
        }
      ]
    }
  ]
}